import React, { useState, useEffect, useRef, useCallback } from "react";
import { useNavigate } from "react-router-dom";
import { Button } from "../../components/shared/Button";
import {
  usePostureTracking,
  resetPostureBaseline,
} from "../../hooks/usePostureTracking";
import { encodeWAV } from "../../utils/encodeWAV";
import { useAuth } from "../../contexts/AuthContext";

interface Question {
  id: string;
  text: string;
  type: string;
  difficulty: string;
  audio_url?: string;
}

const API_BASE = import.meta.env.VITE_API_BASE_URL;
// const WS_BASE = API_BASE.replace("http", "ws"); // Convert http(s) to ws(s) for WebSocket URL
const MAX_ANSWER_DURATION = 90;
const S3_BASE_URL = "https://knok-tts.s3.ap-northeast-2.amazonaws.com/";

export const InterviewSession = () => {
  const navigate = useNavigate();
  const videoRef = useRef<HTMLVideoElement>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const resumeRef = useRef<string>("");
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const questionVideoChunksRef = useRef<Blob[]>([]);
  const wsRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const audioChunksRef = useRef<Float32Array[]>([]);
  const recordTimerRef = useRef<number | null>(null);
  const timeoutRef = useRef<number | null>(null);
  const transcriptRef = useRef<string>("");
  const interviewStartRef = useRef<number>(0);
  const questionStartTimeRef = useRef<number>(0);
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null);
  
  // ì‹¤í–‰ ìƒíƒœ ê´€ë¦¬ë¥¼ ìœ„í•œ ref ì¶”ê°€
  const isProcessingRef = useRef<boolean>(false);
  const isHandlingNextRef = useRef<boolean>(false);
  const isStoppingRecordingRef = useRef<boolean>(false);

  // ì˜¤ë””ì˜¤ ê´€ë¦¬ ìƒíƒœ
  const audioPlaybackRef = useRef<{
    currentAudio: HTMLAudioElement | null;
    isPlaying: boolean;
    playPromise: Promise<void> | null;
  }>({
    currentAudio: null,
    isPlaying: false,
    playPromise: null,
  });

  const auth = useAuth();
  const videoIdRef = useRef(
    `interview_${auth.userEmail || "anonymous"}_${Date.now()}`
  );
  const videoId = videoIdRef.current;

  const [micConnected, setMicConnected] = useState(false);
  const [micLevel, setMicLevel] = useState(0);
  const [isInterviewActive, setIsInterviewActive] = useState(false);
  const [questions, setQuestions] = useState<Question[]>([]);
  const [qIdx, setQIdx] = useState(0);
  const [recordTime, setRecordTime] = useState(0);
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState("");
  const [resumeText, setResumeText] = useState("");
  const [isLoading, setIsLoading] = useState(false);
  const [isPreparing, setIsPreparing] = useState(false);
  const [uploadId, setUploadId] = useState<string | null>(null);
  const [difficulty, setDifficulty] = useState<"ì‰¬ì›€" | "ì¤‘ê°„" | "ì–´ë ¤ì›€">(
    "ì¤‘ê°„"
  );
  const [isPlayingAudio, setIsPlayingAudio] = useState(false);
  const [audioError, setAudioError] = useState<string | null>(null);
  const [userInteracted, setUserInteracted] = useState(false);

  const { countsRef, segmentsRef } = usePostureTracking(
    videoRef,
    videoId,
    questionStartTimeRef.current
  );

  // ì˜¤ë””ì˜¤ ì •ì§€ í•¨ìˆ˜
  const stopCurrentAudio = useCallback(async () => {
    const { currentAudio, playPromise } = audioPlaybackRef.current;
    
    if (playPromise) {
      try {
        await playPromise;
      } catch (error) {
        // ì´ë¯¸ ì¤‘ë‹¨ëœ ê²½ìš° ë¬´ì‹œ
      }
    }
    
    if (currentAudio) {
      currentAudio.pause();
      currentAudio.currentTime = 0;
      currentAudio.removeEventListener('ended', handleAudioEnded);
      currentAudio.removeEventListener('error', handleAudioError);
    }
    
    audioPlaybackRef.current = {
      currentAudio: null,
      isPlaying: false,
      playPromise: null,
    };
    
    setIsPlayingAudio(false);
    setAudioError(null);
  }, []);

  // ì˜¤ë””ì˜¤ ì¢…ë£Œ í•¸ë“¤ëŸ¬
  const handleAudioEnded = useCallback(() => {
    setIsPlayingAudio(false);
    audioPlaybackRef.current.isPlaying = false;
    audioPlaybackRef.current.currentAudio = null;
    audioPlaybackRef.current.playPromise = null;
    
    // ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ í›„ ì•ˆì „í•˜ê²Œ ë…¹ìŒ ì‹œì‘
    if (isInterviewActive && !isRecording && !isPreparing && !isStoppingRecordingRef.current && !isProcessingRef.current) {
      setTimeout(() => {
        // ë‹¤ì‹œ í•œ ë²ˆ ìƒíƒœ í™•ì¸
        if (isInterviewActive && !isRecording && !isPreparing && !isStoppingRecordingRef.current && !isProcessingRef.current) {
          startRecording();
        }
      }, 1000); // ì¶©ë¶„í•œ ì§€ì—°ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
    }
  }, [isInterviewActive, isRecording, isPreparing]);

  // ì˜¤ë””ì˜¤ ì—ëŸ¬ í•¸ë“¤ëŸ¬
  const handleAudioError = useCallback((event: Event) => {
    const audio = event.target as HTMLAudioElement;
    const error = audio.error;
    
    let errorMessage = "ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.";
    if (error) {
      switch (error.code) {
        case error.MEDIA_ERR_SRC_NOT_SUPPORTED:
          errorMessage = "ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.";
          break;
        case error.MEDIA_ERR_NETWORK:
          errorMessage = "ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë¡œ ì˜¤ë””ì˜¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.";
          break;
        case error.MEDIA_ERR_DECODE:
          errorMessage = "ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤.";
          break;
        case error.MEDIA_ERR_ABORTED:
          errorMessage = "ì˜¤ë””ì˜¤ ì¬ìƒì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.";
          break;
      }
    }
    
    console.error("âŒ ì˜¤ë””ì˜¤ ì—ëŸ¬:", errorMessage);
    setAudioError(errorMessage);
    setIsPlayingAudio(false);
    audioPlaybackRef.current.isPlaying = false;
  }, []);

  // ì•ˆì „í•œ ì˜¤ë””ì˜¤ ì¬ìƒ í•¨ìˆ˜
  const playAudioSafely = useCallback(async (audioUrl: string): Promise<boolean> => {
    if (!userInteracted) {
      setAudioError("ë¸Œë¼ìš°ì € ì •ì±…ìƒ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© í›„ ì˜¤ë””ì˜¤ë¥¼ ì¬ìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.");
      return false;
    }

    try {
      // ê¸°ì¡´ ì˜¤ë””ì˜¤ ì •ì§€
      await stopCurrentAudio();
      
      // ìƒˆ ì˜¤ë””ì˜¤ ìƒì„±
      const audio = new Audio();
      audio.preload = 'auto';
      audio.crossOrigin = 'anonymous';
      
      // ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ë“±ë¡
      audio.addEventListener('ended', handleAudioEnded);
      audio.addEventListener('error', handleAudioError);
      
      // ì˜¤ë””ì˜¤ ìƒíƒœ ì—…ë°ì´íŠ¸
      audioPlaybackRef.current.currentAudio = audio;
      setIsPlayingAudio(true);
      setAudioError(null);
      
      // ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì¬ìƒ
      audio.src = audioUrl;
      
      const playPromise = audio.play();
      audioPlaybackRef.current.playPromise = playPromise;
      audioPlaybackRef.current.isPlaying = true;
      
      await playPromise;
      
      return true;
      
    } catch (error) {
      console.error("âŒ ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨:", error);
      
      let errorMessage = "ì˜¤ë””ì˜¤ ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.";
      if (error instanceof DOMException) {
        switch (error.name) {
          case 'NotAllowedError':
            errorMessage = "ë¸Œë¼ìš°ì €ì—ì„œ ì˜¤ë””ì˜¤ ìë™ì¬ìƒì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.";
            break;
          case 'NotSupportedError':
            errorMessage = "ì§€ì›ë˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ì…ë‹ˆë‹¤.";
            break;
          case 'AbortError':
            errorMessage = "ì˜¤ë””ì˜¤ ì¬ìƒì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.";
            break;
        }
      }
      
      setAudioError(errorMessage);
      setIsPlayingAudio(false);
      audioPlaybackRef.current.isPlaying = false;
      return false;
    }
  }, [userInteracted, stopCurrentAudio, handleAudioEnded, handleAudioError]);

  // ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ê°ì§€
  useEffect(() => {
    const handleUserInteraction = () => {
      setUserInteracted(true);
      document.removeEventListener('click', handleUserInteraction);
      document.removeEventListener('keydown', handleUserInteraction);
      document.removeEventListener('touchstart', handleUserInteraction);
    };

    document.addEventListener('click', handleUserInteraction);
    document.addEventListener('keydown', handleUserInteraction);
    document.addEventListener('touchstart', handleUserInteraction);

    return () => {
      document.removeEventListener('click', handleUserInteraction);
      document.removeEventListener('keydown', handleUserInteraction);
      document.removeEventListener('touchstart', handleUserInteraction);
    };
  }, []);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì˜¤ë””ì˜¤ ì •ë¦¬
  useEffect(() => {
    return () => {
      // ëª¨ë“  íƒ€ì´ë¨¸ ì •ë¦¬
      if (recordTimerRef.current) clearInterval(recordTimerRef.current);
      if (timeoutRef.current) clearTimeout(timeoutRef.current);
      
      // ëª¨ë“  í”Œë˜ê·¸ ì´ˆê¸°í™”
      isProcessingRef.current = false;
      isHandlingNextRef.current = false;
      isStoppingRecordingRef.current = false;
      
      // WebSocket ì •ë¦¬
      if (wsRef.current) {
        wsRef.current.close();
        wsRef.current = null;
      }
      
      // ì˜¤ë””ì˜¤ ì •ë¦¬
      stopCurrentAudio();
      
      // ì˜¤ë””ì˜¤ ë…¸ë“œ ì •ë¦¬
      if (sourceRef.current && processorRef.current) {
        try {
          sourceRef.current.disconnect(processorRef.current);
          processorRef.current.disconnect();
        } catch (e) {
          // ì´ë¯¸ ì •ë¦¬ëœ ê²½ìš° ë¬´ì‹œ
        }
        sourceRef.current = null;
        processorRef.current = null;
      }
      
      // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
      if (audioContextRef.current) {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }
      
      // MediaRecorder ì •ë¦¬
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        try {
          mediaRecorderRef.current.stop();
        } catch (e) {
          // ì´ë¯¸ ì •ë¦¬ëœ ê²½ìš° ë¬´ì‹œ
        }
        mediaRecorderRef.current = null;
      }
      
      // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
        streamRef.current = null;
      }
    };
  }, [stopCurrentAudio]);

  // Float32 PCM â†’ Int16 PCM ë³€í™˜
  const convertFloat32ToInt16 = (buffer: Float32Array): Uint8Array => {
    const result = new Int16Array(buffer.length);
    for (let i = 0; i < buffer.length; i++) {
      const s = Math.max(-1, Math.min(1, buffer[i]));
      result[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
    }
    return new Uint8Array(result.buffer);
  };

  // ì´ˆê¸° ì¹´ë©”ë¼/ë§ˆì´í¬ ì…‹ì—…
  useEffect(() => {
    setRecordTime(0);
    let analyser: AnalyserNode;
    let animId: number;
    let mediaStream: MediaStream | null = null;

    const setupMedia = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: true,
          audio: { channelCount: 1, sampleRate: 16000, sampleSize: 16 },
        });
        if (videoRef.current) videoRef.current.srcObject = stream;
        streamRef.current = stream;
        mediaStream = stream;
        setMicConnected(true);

        const AudioCtx =
          (window as any).AudioContext || (window as any).webkitAudioContext;
        if (!AudioCtx) return alert("AudioContext ë¯¸ì§€ì›");
        const audioCtx = new AudioCtx({ sampleRate: 16000 });
        audioContextRef.current = audioCtx;
        if (audioCtx.state === "suspended") {
          console.log("ğŸ”„ ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì¬ì‹œì‘ ì¤‘");
          await audioCtx.resume();
        }

        const source = audioCtx.createMediaStreamSource(stream);
        analyser = audioCtx.createAnalyser();
        analyser.fftSize = 256;
        source.connect(analyser);
        const dataArray = new Uint8Array(analyser.frequencyBinCount);

        const draw = () => {
          analyser.getByteFrequencyData(dataArray);
          const avg =
            dataArray.reduce((sum, v) => sum + v, 0) / dataArray.length;
          setMicLevel(Math.min(100, (avg / 255) * 100));
          animId = requestAnimationFrame(draw);
        };
        draw();
      } catch (err) {
        console.error("getUserMedia error:", err);
        navigate("/interview/check-environment");
      }
    };

    setupMedia();
    return () => {
      cancelAnimationFrame(animId);
      audioContextRef.current?.close();
      mediaStream?.getTracks().forEach((t) => t.stop());
    };
  }, [navigate]);

  // ë©´ì ‘ ì‹œì‘ í•¸ë“¤ëŸ¬
  const onStart = async () => {
    const token = auth.token;
    if (!token) return alert("ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.");
    
    // ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë“±ë¡
    setUserInteracted(true);
    setIsLoading(true);
    try {
      // ì§ˆë¬¸ ë° TTS ìŒì„± ìƒì„± ìš”ì²­
      const generateRes = await fetch(
        `${API_BASE}/generate-resume-questions/`,
        {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${token}`,
          },
          body: JSON.stringify({ difficulty }),
        }
      );
      if (!generateRes.ok) {
        throw new Error(
          `ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: ${
            generateRes.statusText || String(generateRes.status)
          }`
        );
      }
      // Frontend now fetches all questions after generation, no longer relies on WebSocket push for initial set
      const qRes = await fetch(`${API_BASE}/get_all_questions/`, {
        headers: { Authorization: `Bearer ${token}` },
      });
      if (!qRes.ok) throw new Error(qRes.statusText || String(qRes.status));
      const { questions: questionMap } = await qRes.json();

      const email = auth.userEmail ? auth.userEmail.split("@")[0] : "anonymous";
      const filteredQuestionList = (
        Object.entries(questionMap) as [string, string][]
      ).map(([id, text]) => ({
        id,
        text,
        type: "behavioral",
        difficulty: "medium",
        audio_url: `${S3_BASE_URL}${email}/${id}.wav`,
      }));

      // ìê¸°ì†Œê°œ ì§ˆë¬¸ ë§¨ ì•ìœ¼ë¡œ
      const sortedQuestionList = [...filteredQuestionList].sort((a, b) => {
        if (a.text.includes("ìê¸°ì†Œê°œ")) return -1;
        if (b.text.includes("ìê¸°ì†Œê°œ")) return 1;
        const getNumericId = (id: string) => {
          const match = id.match(/\d+/);
          return match ? parseInt(match[0]) : Number.MAX_SAFE_INTEGER;
        };
        return getNumericId(a.id) - getNumericId(b.id);
      });

      setQuestions(sortedQuestionList);

      // ë©´ì ‘ í™œì„±í™”
      setIsInterviewActive(true);
      
      // ì²« ë²ˆì§¸ ì§ˆë¬¸ ì˜¤ë””ì˜¤ ì¬ìƒ
      if (sortedQuestionList.length > 0 && sortedQuestionList[0].audio_url) {
        setTimeout(async () => {
          const success = await playAudioSafely(sortedQuestionList[0].audio_url!);
          if (!success) {
            // ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ë…¹ìŒ ì‹œì‘
            setTimeout(() => {
              if (isInterviewActive && !isRecording) {
                startRecording();
              }
            }, 1000);
          }
        }, 1000); // ë©´ì ‘ ì‹œì‘ í›„ ì•½ê°„ì˜ ì§€ì—°
      }

      // ì´ë ¥ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
      try {
        const rRes = await fetch(`${API_BASE}/get-resume-text/`, {
          headers: { Authorization: `Bearer ${token}` },
        });
        if (rRes.ok) {
          const { resume_text } = await rRes.json();
          setResumeText(resume_text || "");
          resumeRef.current = resume_text || "";
        }
      } catch (resumeError) {
        console.error("ì´ë ¥ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨:", resumeError);
      }

      setQIdx(0);
      interviewStartRef.current = Date.now();
      questionStartTimeRef.current = Date.now();
    } catch (err) {
      console.error("ë©´ì ‘ ì‹œì‘ ì‹¤íŒ¨:", err);
      alert("ë©´ì ‘ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.");
    } finally {
      setIsLoading(false);
    }
  };

  // ê¼¬ë¦¬ì§ˆë¬¸ íŒë‹¨
  const decideFollowup = async (
    userAnswer: string,
    questionIndex: number
  ): Promise<boolean> => {
    const token = auth.token;
    if (!token || !resumeRef.current) return false;
    const payload = {
      resume_text: resumeRef.current,
      user_answer: userAnswer.trim(),
      base_question_number: parseInt(
        questions[questionIndex].id.match(/\d+/)?.[0] || "0",
        10
      ),
      interview_id: videoId,
      existing_question_numbers: questions.map((q) => q.id),
    };
    console.log('[ê¼¬ë¦¬ì§ˆë¬¸ ìš”ì²­]', payload);
    const res = await fetch(`${API_BASE}/decide_followup_question/`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${token}`,
      },
      body: JSON.stringify(payload),
    });
    console.log('[ê¼¬ë¦¬ì§ˆë¬¸ ì‘ë‹µ]', res.status, res.statusText);
    if (!res.ok) return false;
    const data = await res.json();
    console.log('[ê¼¬ë¦¬ì§ˆë¬¸ ë°ì´í„°]', data);
    return data.followup_generated; // Return whether a followup was generated
  };

  // ë…¹ìŒ ë° WebSocket ì‹œì‘
  const startRecording = async () => {
    if (!questions[qIdx] || !streamRef.current) {
      return;
    }

    // ê°•ë ¥í•œ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
    if (isRecording || isPreparing || isStoppingRecordingRef.current || isProcessingRef.current || isHandlingNextRef.current) {
      return;
    }
    
    resetPostureBaseline();
    setRecordTime(0);
    setIsRecording(true);
    setIsPreparing(false);

    const token = auth.token;
    const wsUrl = `${import.meta.env.VITE_WEBSOCKET_BASE_URL}/ws/transcribe?email=${
      auth.userEmail
    }&question_id=${questions[qIdx].id}&token=${token}`;
    
    const ws = new WebSocket(wsUrl);
    ws.binaryType = "arraybuffer";
    wsRef.current = ws;

    ws.onopen = async () => {
      try {
        const audioCtx = audioContextRef.current!;
        if (audioCtx.state === "suspended") {
          await audioCtx.resume();
        }

        const source = audioCtx.createMediaStreamSource(streamRef.current!);
        sourceRef.current = source;
        const processor = audioCtx.createScriptProcessor(4096, 1, 1);
        processorRef.current = processor;
        
        processor.onaudioprocess = (e) => {
          if (!isRecording || ws.readyState !== WebSocket.OPEN) {
            return;
          }
          
          const floatData = e.inputBuffer.getChannelData(0);
          const pcm = convertFloat32ToInt16(floatData);
          
          try {
            ws.send(pcm);
            audioChunksRef.current.push(new Float32Array(floatData));
          } catch (sendError) {
            console.error("âŒ WebSocket send failed:", sendError);
          }
        };
        
        source.connect(processor);
        processor.connect(audioCtx.destination);

        // ë…¹ìŒ íƒ€ì´ë¨¸ ì‹œì‘
        recordTimerRef.current = window.setInterval(() => {
          setRecordTime((prev) => {
            const newTime = Math.min(prev + 1, MAX_ANSWER_DURATION);
            if (newTime >= MAX_ANSWER_DURATION) {
              clearInterval(recordTimerRef.current!);
              stopRecording();
            }
            return newTime;
          });
        }, 1000);

        // ìµœëŒ€ ë…¹ìŒ ì‹œê°„ íƒ€ì„ì•„ì›ƒ
        timeoutRef.current = window.setTimeout(async () => {
          clearInterval(recordTimerRef.current!);
          await stopRecording();
        }, MAX_ANSWER_DURATION * 1000);
        
      } catch (audioError) {
        console.error("âŒ Audio setup failed:", audioError);
        setIsRecording(false);
        ws.close();
      }
    };

    ws.onmessage = (ev) => {
      try {
        const data = JSON.parse(ev.data);
        if (data.type === "upload_id") {
          setUploadId(data.upload_id);
          return;
        }
        if (data.transcript) {
          setTranscript((prev) => {
            const updated = prev + data.transcript + "\n";
            transcriptRef.current = updated;
            return updated;
          });
        }
      } catch (parseError) {
        // Ignore parse errors - they're usually not critical
      }
    };
    
    ws.onerror = (e) => {
      console.error("âŒ WebSocket error:", e);
      setIsRecording(false);
    };
    
    ws.onclose = (event) => {
      setIsRecording(false);
    };
  };

  // ë…¹ìŒ ì¢…ë£Œ, ì—…ë¡œë“œ, ê¼¬ë¦¬ì§ˆë¬¸ íŒë‹¨
  const stopRecording = async () => {
    // ê°•ë ¥í•œ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
    if (isStoppingRecordingRef.current || !isRecording || isPreparing) {
      return;
    }
    
    isStoppingRecordingRef.current = true;
    
    try {
      // íƒ€ì´ë¨¸ ì •ë¦¬
      if (recordTimerRef.current) {
        clearInterval(recordTimerRef.current);
        recordTimerRef.current = null;
      }
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
        timeoutRef.current = null;
      }
      
      setIsRecording(false);
      setIsPreparing(true);

      // MediaRecorder ì •ë¦¬
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        try {
          mediaRecorderRef.current.stop();
        } catch (recorderError) {
          console.error("âŒ MediaRecorder stop error:", recorderError);
        }
      }

      // WebSocket ì •ë¦¬
      if (wsRef.current?.readyState === WebSocket.OPEN) {
        try {
          wsRef.current.send(new TextEncoder().encode("END"));
        } catch (wsError) {
          console.error("âŒ WebSocket END signal error:", wsError);
        }
        wsRef.current.close();
      }
      
      // Audio nodes ì •ë¦¬
      if (sourceRef.current && processorRef.current) {
        try {
          sourceRef.current.disconnect(processorRef.current);
          processorRef.current.disconnect();
        } catch (disconnectError) {
          console.error("âŒ Audio node disconnect error:", disconnectError);
        }
        sourceRef.current = null;
        processorRef.current = null;
      }

      // í˜„ì¬ ë°ì´í„° ìˆ˜ì§‘
      const currentAudioChunks = [...audioChunksRef.current];
      const currentTranscript = transcriptRef.current;
      const currentVideoChunks = [...questionVideoChunksRef.current];
      const currentRecordTime = recordTime;
      const currentSegments = [...segmentsRef.current];
      const currentUploadId = uploadId;

      // Refs ì´ˆê¸°í™”
      audioChunksRef.current = [];
      transcriptRef.current = "";
      questionVideoChunksRef.current = [];
      segmentsRef.current = [];

      // ì ì‹œ ëŒ€ê¸° í›„ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬
      setTimeout(() => {
        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¹„ë™ê¸° ì²˜ë¦¬
        processRecordingData(
          currentAudioChunks,
          currentTranscript,
          currentVideoChunks,
          currentRecordTime,
          currentSegments,
          currentUploadId,
          auth.userEmail!,
          questions[qIdx].id,
          videoId,
          auth.token!
        ).catch(error => {
          console.error("âŒ Background processing error:", error);
          // ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì§„í–‰
          setTimeout(() => {
            if (!isHandlingNextRef.current && !isProcessingRef.current) {
              handleNext();
            }
          }, 1000);
        });
      }, 500);
      
    } catch (error) {
      console.error("âŒ Stop recording error:", error);
    } finally {
      // í”Œë˜ê·¸ í•´ì œ
      setTimeout(() => {
        isStoppingRecordingRef.current = false;
      }, 1000);
    }
  };

  const processRecordingData = async (
    audioChunks: Float32Array[],
    transcript: string,
    videoChunks: Blob[],
    duration: number,
    segments: { start: number; end: number }[],
    currentUploadId: string | null,
    userEmail: string,
    questionId: string,
    interviewId: string,
    token: string
  ) => {
    // ê°•ë ¥í•œ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
    if (isProcessingRef.current || isPreparing) {
      return;
    }
    
    isProcessingRef.current = true;
    setIsPreparing(true);
    
    try {
      // transcriptê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ì¦‰ì‹œ ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ
      const refinedTranscript = transcript.trim();
      if (
        !refinedTranscript ||
        refinedTranscript.toLowerCase() === "blob" ||
        refinedTranscript.length <= 5
      ) {
        setTimeout(() => {
          isProcessingRef.current = false;
          setIsPreparing(false);
          if (!isHandlingNextRef.current) {
            handleNext();
          }
        }, 1000);
        return;
      }

      // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì—…ë¡œë“œ ì‘ì—… ìˆ˜í–‰ (ì—ëŸ¬ê°€ ë‚˜ë„ ê³„ì† ì§„í–‰)
      const uploadPromises = [];

      // Video clip upload
      if (videoChunks.length > 0) {
        const videoUploadPromise = (async () => {
          try {
            const videoBlob = new Blob(videoChunks, { type: "video/webm" });
            const videoFile = new File([videoBlob], "clip.webm", { type: "video/webm" });
            const clipForm = new FormData();
            clipForm.append("video", videoFile);
            clipForm.append("interview_id", interviewId);
            clipForm.append("question_id", questionId);
            
            const response = await fetch(`${API_BASE}/video/upload-question-clip/`, {
              method: "POST",
              headers: { Authorization: `Bearer ${token}` },
              body: clipForm,
            });
            
            if (!response.ok) {
              console.error("âŒ Video upload failed:", response.status);
            }
          } catch (error) {
            console.error("âŒ Video upload error:", error);
          }
        })();
        uploadPromises.push(videoUploadPromise);
      }

      // Audio upload
      if (audioChunks.length > 0) {
        const audioUploadPromise = (async () => {
          try {
            const wavBlob = encodeWAV(
              audioChunks.reduce((acc, cur) => {
                const tmp = new Float32Array(acc.length + cur.length);
                tmp.set(acc);
                tmp.set(cur, acc.length);
                return tmp;
              }, new Float32Array()),
              16000
            );
            
            const audioForm = new FormData();
            audioForm.append("audio", new File([wavBlob], "answer.wav", { type: "audio/wav" }));
            audioForm.append("transcript", new Blob([refinedTranscript], { type: "text/plain" }));
            audioForm.append("email", userEmail);
            audioForm.append("question_id", questionId);
            if (currentUploadId) {
              audioForm.append("upload_id", currentUploadId);
            }
            
            const response = await fetch(`${API_BASE}/audio/upload/`, {
              method: "POST",
              headers: { Authorization: `Bearer ${token}` },
              body: audioForm,
            });
            
            if (!response.ok) {
              console.error("âŒ Audio upload failed:", response.status);
            }
          } catch (error) {
            console.error("âŒ Audio upload error:", error);
          }
        })();
        uploadPromises.push(audioUploadPromise);
      }

      // ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ëŠ” ê±´ë„ˆë›°ê¸° (500 ì—ëŸ¬ ë°œìƒ ì›ì¸)
      // const relSegments = segments.filter(...);
      // if (relSegments.length > 0) { ... }

      // ì—…ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
      await Promise.allSettled(uploadPromises);

      // ê¼¬ë¦¬ì§ˆë¬¸ íŒë‹¨
      try {
        const shouldFollowup = await Promise.race([
          decideFollowup(refinedTranscript, qIdx),
          new Promise((_, reject) => setTimeout(() => reject(new Error('Timeout')), 5000))
        ]);
        
        if (shouldFollowup) {
          const qRes = await fetch(`${API_BASE}/get_all_questions/`, {
            headers: { Authorization: `Bearer ${token}` },
          });
          
          if (qRes.ok) {
            const { questions: questionMap } = await qRes.json();
            const emailPrefix = userEmail.split("@")[0];
            const updatedQuestionList = (
              Object.entries(questionMap) as [string, string][]
            ).map(([id, text]) => ({
              id,
              text,
              type: "behavioral",
              difficulty: "medium",
              audio_url: `${S3_BASE_URL}${emailPrefix}/${id}.wav`,
            }));

            const sortedUpdatedQuestionList = [...updatedQuestionList].sort((a, b) => {
              const getNumericId = (id: string) => {
                const match = id.match(/\d+/);
                return match ? parseInt(match[0]) : Number.MAX_SAFE_INTEGER;
              };
              return getNumericId(a.id) - getNumericId(b.id);
            });

            setQuestions(sortedUpdatedQuestionList);
            
            const nextQIndex = sortedUpdatedQuestionList.findIndex(
              (q) => q.id.startsWith(questions[qIdx].id + "-")
            ); 
            
            if (nextQIndex !== -1) {
              setQIdx(nextQIndex);
            } else {
              setQIdx(prev => prev + 1);
            }
            
            isProcessingRef.current = false;
            setIsPreparing(false);
            return;
          }
        }
      } catch (followupError) {
        console.error("âŒ Followup decision error:", followupError);
      }
      
      // ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì§„í–‰
      setTimeout(() => {
        isProcessingRef.current = false;
        setIsPreparing(false);
        if (!isHandlingNextRef.current) {
          handleNext();
        }
      }, 1000);

    } catch (error) {
      console.error("âŒ Processing error:", error);
      setTimeout(() => {
        isProcessingRef.current = false;
        setIsPreparing(false);
        if (!isHandlingNextRef.current) {
          handleNext();
        }
      }, 2000);
    }
  };

  // ë©´ì ‘ ì¢…ë£Œ
  const endInterview = async () => {
    setIsLoading(true);
    const token = auth.token; // Use auth.token
    if (!token) return;

    // Final full interview video processing (ì´ì œ ì „ì²´ ì˜ìƒ ì—…ë¡œë“œ ì•ˆí•¨)
    if (!uploadId) {
      console.warn(
        "Upload IDê°€ ì—†ì–´ ìµœì¢… ë¶„ì„ì„ ê±´ë„ˆë›°ê³  í”¼ë“œë°± í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤."
      );
    }

    // Existing posture analysis upload remains (countsRef.current)
    // This part should still be done to analyze voice data after last question
    if (uploadId && countsRef.current) {
      try {
        console.log("ë¶„ì„ ìš”ì²­ ì „ uploadId:", uploadId);
        console.log("ë¶„ì„ ìš”ì²­ ì „ posture_count:", countsRef.current);
        const analyzePayload = {
          upload_id: uploadId,
          posture_count: countsRef.current,
        };
        console.log("â–¶ Final analyze-voice ìš”ì²­ ë°ì´í„°:", analyzePayload);

        const r2 = await fetch(`${API_BASE}/analyze-voice/`, {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${token}`,
          },
          body: JSON.stringify(analyzePayload),
        });

        if (!r2.ok) {
          const errorText = r2.statusText || String(r2.status);
          console.error("â–¶ analyze-voice API ì˜¤ë¥˜:", r2.status, errorText);
          throw new Error(`ë¶„ì„ API ì‹¤íŒ¨: ${errorText}`);
        }
        const { analysis } = await r2.json();
        navigate("/interview/feedback", {
          state: {
            upload_id: videoId, // Use videoId as interview_id
            segments: [], // Segments will be fetched in FeedbackReport
            analysis,
            clips: [], // Clips will be fetched in FeedbackReport
          },
        });
      } catch (e) {
        console.error("ìµœì¢… ë¶„ì„ ì‹¤íŒ¨:", e);
        console.error("ì‹¤íŒ¨ ë‹¹ì‹œ uploadId:", uploadId);
        console.error("ì‹¤íŒ¨ ë‹¹ì‹œ posture_count:", countsRef.current);
        alert("ìµœì¢… ë¶„ì„ ì‹¤íŒ¨");
      }
    } else {
      navigate("/interview/feedback", {
        state: {
          upload_id: videoId,
          segments: [],
          analysis: {},
          clips: [],
        },
      });
    }

    setQuestions([]);
    setQIdx(0);
    setIsInterviewActive(false);
    setTranscript("");
    audioChunksRef.current = [];

    if (audioRef.current) {
      audioRef.current.pause();
      setIsPlayingAudio(false);
    }
  };

  // ë‹¤ìŒ ì§ˆë¬¸ í˜¹ì€ ë©´ì ‘ ì¢…ë£Œ
  const handleNext = async () => {
    // ê°•ë ¥í•œ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
    if (isHandlingNextRef.current || isPreparing || isLoading) {
      return;
    }
    
    isHandlingNextRef.current = true;
    
    try {
      // ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë“±ë¡
      setUserInteracted(true);
      
      // í˜„ì¬ ë…¹ìŒ ì¤‘ì´ë©´ ì¤‘ë‹¨
      if (isRecording) {
        await stopRecording();
        return; // stopRecordingì—ì„œ ë‹¤ì‹œ handleNextê°€ í˜¸ì¶œë¨
      }
      
      // í˜„ì¬ ì˜¤ë””ì˜¤ ì •ì§€ (ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ë‹¨ ë¬¸ì œ í•´ê²°)
      if (isPlayingAudio) {
        await stopCurrentAudio();
        // ì˜¤ë””ì˜¤ ì¤‘ë‹¨ í›„ ì ì‹œ ëŒ€ê¸°
        await new Promise(resolve => setTimeout(resolve, 500));
      }
      
      if (qIdx < questions.length - 1) {
        resetPostureBaseline();
        setQIdx((prev) => prev + 1);
        setTranscript("");
        audioChunksRef.current = [];

        // ë¹„ë””ì˜¤ ë…¹í™” ì‹œì‘
        if (streamRef.current) {
          questionVideoChunksRef.current = [];
          try {
            const newRecorder = new MediaRecorder(streamRef.current, {
              mimeType: "video/webm",
            });
            newRecorder.ondataavailable = (e) => {
              if (e.data.size > 0) questionVideoChunksRef.current.push(e.data);
            };
            newRecorder.start();
            mediaRecorderRef.current = newRecorder;
            questionStartTimeRef.current = Date.now();
          } catch (recorderError) {
            console.error("âŒ MediaRecorder creation failed:", recorderError);
          }
        }

        // ë‹¤ìŒ ì§ˆë¬¸ ì˜¤ë””ì˜¤ ì¬ìƒ
        const nextQuestion = questions[qIdx + 1];
        if (nextQuestion?.audio_url) {
          // ì˜¤ë””ì˜¤ ì¬ìƒ ì „ ì ì‹œ ëŒ€ê¸°
          setTimeout(async () => {
            if (isHandlingNextRef.current) { // ì—¬ì „íˆ ì²˜ë¦¬ ì¤‘ì¸ ê²½ìš°ë§Œ ì‹¤í–‰
              const success = await playAudioSafely(nextQuestion.audio_url!);
              if (!success) {
                // ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ë…¹ìŒ ì‹œì‘
                setTimeout(() => {
                  if (isInterviewActive && !isRecording && !isPreparing) {
                    startRecording();
                  }
                }, 1000);
              }
            }
          }, 1000);
        } else {
          // ì˜¤ë””ì˜¤ URLì´ ì—†ëŠ” ê²½ìš° ì¦‰ì‹œ ë…¹ìŒ ì‹œì‘
          setTimeout(() => {
            if (isInterviewActive && !isRecording && !isPreparing) {
              startRecording();
            }
          }, 1000);
        }
      } else {
        endInterview();
      }
    } catch (error) {
      console.error("âŒ ì§ˆë¬¸ ì „í™˜ ì¤‘ ì˜¤ë¥˜:", error);
    } finally {
      // ì²˜ë¦¬ ì™„ë£Œ í›„ í”Œë˜ê·¸ í•´ì œ
      setTimeout(() => {
        isHandlingNextRef.current = false;
      }, 2000);
    }
  };

  return (
    <div className="pt-[92px] relative min-h-screen bg-gray-900 text-white">
      <div className="max-w-7xl mx-auto px-4 py-8 grid grid-cols-1 md:grid-cols-3 gap-8">
        <div className="md:col-span-2">
          <div className="relative aspect-video bg-black rounded-lg overflow-hidden">
            <video
              ref={videoRef}
              autoPlay
              muted
              playsInline
              className="w-full h-full object-cover"
            />
            <div className="absolute top-4 left-4 flex flex-col items-start space-y-2 bg-black bg-opacity-50 px-3 py-2 rounded-lg">
              <div>
                <span className="text-xs mr-2">ë§ˆì´í¬ ìƒíƒœ:</span>
                <span
                  className={micConnected ? "text-green-400" : "text-red-400"}
                >
                  {micConnected ? "ì—°ê²°ë¨" : "ë¯¸ì—°ê²°"}
                </span>
              </div>
              <div className="w-32 h-2 bg-gray-600 rounded overflow-hidden">
                <div
                  className="h-full bg-green-400"
                  style={{ width: `${micLevel}%` }}
                />
              </div>
            </div>
          </div>
        </div>
        <div className="space-y-6">
          {!isInterviewActive ? (
            <div className="bg-gray-800 p-6 rounded-lg">
              <h2 className="text-xl font-semibold mb-4">ë©´ì ‘ ì¤€ë¹„</h2>
              <p className="text-gray-400 mb-6">
                ì´ë ¥ì„œ ê¸°ë°˜ ì§ˆë¬¸ì„ ê°€ì ¸ì˜¤ê³  ë…¹ìŒì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
              </p>
              <div className="mb-6">
                <h3 className="text-sm font-medium text-gray-300 mb-2">
                  ì§ˆë¬¸ ë‚œì´ë„ ì„ íƒ
                </h3>
                <div className="flex gap-2">
                  {["ì‰¬ì›€", "ì¤‘ê°„", "ì–´ë ¤ì›€"].map((level) => (
                    <button
                      key={level}
                      onClick={() =>
                        setDifficulty(level as "ì‰¬ì›€" | "ì¤‘ê°„" | "ì–´ë ¤ì›€")
                      }
                      className={`px-4 py-1 w-16 rounded-full text-sm border text-center transition
                        ${
                          difficulty === level
                            ? "bg-purple-600 text-white border-transparent font-semibold"
                            : "bg-transparent text-gray-300 border-gray-400 hover:bg-gray-600"
                        }
                      `}
                    >
                      {level}
                    </button>
                  ))}
                </div>
              </div>
              <Button
                onClick={onStart}
                className="w-full"
                size="lg"
                disabled={isLoading || !micConnected}
                isLoading={isLoading}
              >
                AI ë©´ì ‘ ì‹œì‘í•˜ê¸°
              </Button>
            </div>
          ) : isPreparing ? (
            <div className="bg-gray-800 p-6 rounded-lg flex flex-col items-center space-y-4">
              <p className="text-gray-300">ë‹¤ìŒ ì§ˆë¬¸ ì¤€ë¹„ ì¤‘â€¦</p>
              <svg
                className="w-10 h-10 animate-spin text-primary"
                viewBox="0 0 24 24"
              >
                <circle
                  className="opacity-25"
                  cx="12"
                  cy="12"
                  r="10"
                  stroke="currentColor"
                  strokeWidth="4"
                />
                <path
                  className="opacity-75"
                  fill="currentColor"
                  d="M4 12a8 8 0 018-8v8H4z"
                />
              </svg>
            </div>
          ) : (
            <div className="bg-gray-800 p-6 rounded-lg">
              <div className="flex justify-between items-center mb-4">
                <h3 className="text-lg font-medium">í˜„ì¬ ì§ˆë¬¸</h3>
                <span className="text-sm text-gray-400">
                  {qIdx + 1}/{questions.length}
                </span>
              </div>
              <p className="text-gray-300">{questions[qIdx]?.text}</p>
              
              {/* ì˜¤ë””ì˜¤ ìƒíƒœ í‘œì‹œ */}
              {isPlayingAudio && (
                <div className="mt-2 flex items-center text-sm text-blue-400">
                  <svg
                    className="w-4 h-4 mr-1 animate-pulse"
                    fill="currentColor"
                    viewBox="0 0 20 20"
                  >
                    <path
                      fillRule="evenodd"
                      d="M9.383 3.076A1 1 0 0110 4v12a1 1 0 01-1.707.707L4.586 13H2a1 1 0 01-1-1V8a1 1 0 011-1h2.586l3.707-3.707a1 1 0 011.09-.217zM14.657 2.929a1 1 0 011.414 0A9.972 9.972 0 0119 10a9.972 9.972 0 01-2.929 7.071a1 1 0 01-1.414-1.414A7.971 7.971 0 0017 10c0-2.21-.894-4.208-2.343-5.657a1 1 0 010-1.414zm-2.829 2.828a1 1 0 011.415 0A5.983 5.983 0 0115 10a5.984 5.984 0 01-1.757 4.243a1 1 0 01-1.415-1.415A3.984 3.984 0 0013 10a3.983 3.983 0 00-1.172-2.828a1 1 0 010-1.415z"
                      clipRule="evenodd"
                    />
                  </svg>
                  ì§ˆë¬¸ ìŒì„± ì¬ìƒ ì¤‘...
                </div>
              )}
              
              {/* ì˜¤ë””ì˜¤ ì—ëŸ¬ í‘œì‹œ */}
              {audioError && (
                <div className="mt-2 p-3 bg-red-900/50 border border-red-500 rounded-lg">
                  <div className="flex items-start space-x-2">
                    <svg className="w-5 h-5 text-red-400 mt-0.5 flex-shrink-0" fill="currentColor" viewBox="0 0 20 20">
                      <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM8.707 7.293a1 1 0 00-1.414 1.414L8.586 10l-1.293 1.293a1 1 0 101.414 1.414L10 11.414l1.293 1.293a1 1 0 001.414-1.414L11.414 10l1.293-1.293a1 1 0 00-1.414-1.414L10 8.586 8.707 7.293z" clipRule="evenodd" />
                    </svg>
                    <div className="flex-1">
                      <p className="text-sm text-red-300 font-medium">ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜</p>
                      <p className="text-xs text-red-400 mt-1">{audioError}</p>
                      {questions[qIdx]?.audio_url && (
                        <button
                          onClick={async () => {
                            setUserInteracted(true);
                            await playAudioSafely(questions[qIdx].audio_url!);
                          }}
                          className="mt-2 text-xs bg-red-600 hover:bg-red-700 text-white px-3 py-1 rounded transition"
                        >
                          ë‹¤ì‹œ ì¬ìƒ
                        </button>
                      )}
                    </div>
                  </div>
                </div>
              )}
              
              {/* ì‚¬ìš©ì ìƒí˜¸ì‘ìš© í•„ìš” ì•Œë¦¼ */}
              {!userInteracted && isInterviewActive && (
                <div className="mt-2 p-3 bg-yellow-900/50 border border-yellow-500 rounded-lg">
                  <div className="flex items-start space-x-2">
                    <svg className="w-5 h-5 text-yellow-400 mt-0.5 flex-shrink-0" fill="currentColor" viewBox="0 0 20 20">
                      <path fillRule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clipRule="evenodd" />
                    </svg>
                    <div className="flex-1">
                      <p className="text-sm text-yellow-300 font-medium">ë¸Œë¼ìš°ì € ìë™ì¬ìƒ ì œí•œ</p>
                      <p className="text-xs text-yellow-400 mt-1">
                        ì˜¤ë””ì˜¤ ì¬ìƒì„ ìœ„í•´ ì•„ë¬´ ë²„íŠ¼ì´ë‚˜ í´ë¦­í•´ì£¼ì„¸ìš”.
                      </p>
                    </div>
                  </div>
                </div>
              )}
              {isRecording && (
                <p className="mt-4 text-sm text-gray-400">
                  ë‚¨ì€ ë‹µë³€ ì‹œê°„: {MAX_ANSWER_DURATION - recordTime}ì´ˆ
                </p>
              )}
              <Button
                variant="outline"
                className="w-full mt-4"
                onClick={handleNext}
                disabled={isLoading || isPlayingAudio}
              >
                {qIdx < questions.length - 1 ? "ë‹¤ìŒ ì§ˆë¬¸" : "ë©´ì ‘ ì¢…ë£Œ"}
              </Button>
            </div>
          )}
        </div>
      </div>
      {isLoading && (
        <div className="fixed inset-0 bg-black/50 flex items-center justify-center z-50">
          <div className="bg-white rounded-lg p-8 text-center max-w-xs mx-4 space-y-4">
            <h3 className="text-gray-900 text-lg font-semibold">
              {isLoading ? "ì²˜ë¦¬ ì¤‘..." : "í”¼ë“œë°± ìƒì„± ì¤‘..."}
            </h3>
            <svg
              className="mx-auto w-12 h-12 animate-spin text-primary"
              viewBox="0 0 24 24"
            >
              <circle
                className="opacity-25"
                cx="12"
                cy="12"
                r="10"
                stroke="currentColor"
                strokeWidth="4"
              />
              <path
                className="opacity-75"
                fill="currentColor"
                d="M4 12a8 8 0 018-8v8H4z"
              />
            </svg>
          </div>
        </div>
      )}
      <audio ref={audioRef} hidden />
    </div>
  );
};
